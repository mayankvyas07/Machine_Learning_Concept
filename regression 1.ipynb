{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1208b4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Simple Linear Regression vs. Multiple Linear Regression:\n",
    "Simple Linear Regression involves predicting a dependent variable (usually denoted as \"y\") using a single independent variable (usually denoted as \"x\"). The relationship between the two variables is assumed to be linear, and the model tries to find the best-fit line that minimizes the sum of squared differences between the observed and predicted values.\n",
    "\n",
    "Example of Simple Linear Regression: Predicting a person's weight (y) based on their height (x).\n",
    "\n",
    "Multiple Linear Regression, on the other hand, involves predicting a dependent variable using multiple independent variables. The relationship is still assumed to be linear, but now the model accounts for the influence of multiple factors on the dependent variable.\n",
    "\n",
    "Example of Multiple Linear Regression: Predicting a house's price (y) based on its size (x1), number of bedrooms (x2), and location (x3).\n",
    "\n",
    "Q2. Assumptions of Linear Regression and Checking Them:\n",
    "\n",
    "Linearity: The relationship between the variables is assumed to be linear.\n",
    "Independence: Residuals (the differences between observed and predicted values) should be independent of each other.\n",
    "Homoscedasticity: The variance of residuals should be constant across all levels of the independent variable(s).\n",
    "Normality: Residuals are assumed to be normally distributed.\n",
    "These assumptions can be checked using various diagnostic techniques like residual plots, normality tests, and the Durbin-Watson statistic.\n",
    "\n",
    "Q3. Interpreting Slope and Intercept:\n",
    "In a simple linear regression equation (y = mx + b), the slope (m) represents the change in the dependent variable (y) for a unit change in the independent variable (x). The intercept (b) is the value of the dependent variable when the independent variable is zero.\n",
    "\n",
    "Example: In a simple linear regression predicting weight (y) from height (x), if the slope is 2.5, it means that, on average, every additional inch in height corresponds to an increase of 2.5 pounds in weight. The intercept would represent the estimated weight when height is zero, which doesn't make sense in this context.\n",
    "\n",
    "Q4. Gradient Descent:\n",
    "Gradient Descent is an optimization algorithm used in machine learning to minimize the error of a model by adjusting its parameters iteratively. It calculates the gradient of the cost function with respect to the model's parameters and updates the parameters in the opposite direction of the gradient to reach the minimum.\n",
    "\n",
    "Q5. Multiple Linear Regression Model:\n",
    "Multiple Linear Regression extends simple linear regression to include multiple independent variables. The model equation becomes y = b0 + b1x1 + b2x2 + ... + bn*xn, where b0 is the intercept, b1 to bn are the coefficients for each independent variable, and x1 to xn are the respective independent variables.\n",
    "\n",
    "Q6. Multicollinearity:\n",
    "Multicollinearity occurs when two or more independent variables in a multiple linear regression model are highly correlated. This can lead to instability in coefficient estimates and difficulty in interpreting their individual effects.\n",
    "\n",
    "Detection: Correlation matrices or variance inflation factor (VIF) values can help identify multicollinearity.\n",
    "\n",
    "Addressing: Removing one of the correlated variables, combining them into a single variable, or using regularization techniques can mitigate multicollinearity.\n",
    "\n",
    "Q7. Polynomial Regression:\n",
    "Polynomial Regression is a form of regression in which the relationship between the independent and dependent variables is modeled as an nth-degree polynomial. This allows for capturing nonlinear relationships.\n",
    "\n",
    "Q8. Advantages and Disadvantages of Polynomial Regression:\n",
    "Advantages:\n",
    "\n",
    "Can model more complex relationships than linear regression.\n",
    "Can fit data with curvilinear patterns.\n",
    "Disadvantages:\n",
    "\n",
    "Prone to overfitting, especially with high-degree polynomials.\n",
    "Harder to interpret than linear regression.\n",
    "May not generalize well to new data.\n",
    "Use Cases:\n",
    "\n",
    "When the relationship between variables seems nonlinear.\n",
    "When you want to capture more intricate patterns in the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
